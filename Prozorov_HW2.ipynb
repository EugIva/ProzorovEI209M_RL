{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EugIva/ProzorovEI209M_RL/blob/main/Prozorov_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Выполнил Прозоров Евгений 209М"
      ],
      "metadata": {
        "id": "4IWvyruFhVd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализуйте алгоритм SAC для среды lunar lander"
      ],
      "metadata": {
        "id": "y9vuAw7AZdbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\""
      ],
      "metadata": {
        "id": "VCWEy3pfMvHA",
        "outputId": "9c0670bf-0a12-4f7a-b7e3-4df004469448",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379366 sha256=91f295b3dc949860b197d84fda67668e9c1afd53b336aaf64be3a6f8d26bcdfe\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import random\n",
        "from torch.distributions import Normal"
      ],
      "metadata": {
        "id": "5jSxwGXvtAWp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GAMMA = 0.99\n",
        "TAU = 0.005\n",
        "ALPHA = 0.2\n",
        "ACTOR_LR = 3e-4\n",
        "CRITIC_LR = 3e-4\n",
        "REPLAY_SIZE = 100000\n",
        "BATCH_SIZE = 256\n",
        "START_STEPS = 10000\n",
        "TOTAL_STEPS = 200000\n",
        "UPDATE_AFTER = 1000\n",
        "UPDATE_EVERY = 50"
      ],
      "metadata": {
        "id": "gBd1ntZM6uaH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, act_limit):  # Добавлен act_limit в параметры\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256), nn.ReLU(),\n",
        "        )\n",
        "        self.mu_layer = nn.Linear(256, act_dim)\n",
        "        self.log_std_layer = nn.Linear(256, act_dim)\n",
        "        self.act_limit = act_limit  # Теперь корректно инициализируется\n",
        "\n",
        "    def forward(self, obs):\n",
        "        x = F.relu(self.net(obs))\n",
        "        mean, std = self.mu_layer(x), torch.clamp(self.log_std_layer(x), -20, 2).exp()\n",
        "        normal = torch.distributions.Normal(mean, std)\n",
        "\n",
        "        x_t = normal.rsample()\n",
        "        y_t = torch.tanh(x_t)\n",
        "        action = y_t * self.act_limit  # Масштабируем в диапазон [-act_limit, act_limit]\n",
        "\n",
        "        log_prob = normal.log_prob(x_t)\n",
        "        log_prob -= torch.log(1 - y_t.pow(2) + 1e-6)\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\n",
        "\n",
        "        return action, log_prob\n",
        "\n",
        "#Если deterministic установлено в True, действие должно быть средним значением распределения, иначе мы используем семплирование\n",
        "    def get_action(self, obs, deterministic=False):\n",
        "        with torch.no_grad():\n",
        "            x = F.relu(self.net(obs))\n",
        "            mean = self.mu_layer(x)\n",
        "            log_std = self.log_std_layer(x)\n",
        "            std = log_std.exp()\n",
        "\n",
        "            if deterministic:\n",
        "                action = torch.tanh(mean)\n",
        "            else:\n",
        "                normal = Normal(mean, std)\n",
        "                x_t = normal.rsample()\n",
        "                action = torch.tanh(x_t)\n",
        "\n",
        "            # Масштабируем действие в диапазон [-act_limit, act_limit]\n",
        "            action = action * self.act_limit\n",
        "            return action"
      ],
      "metadata": {
        "id": "4VVfs61H6xlX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.q1 = nn.Sequential(\n",
        "            nn.Linear(obs_dim + act_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        self.q2 = nn.Sequential(\n",
        "            nn.Linear(obs_dim + act_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs, act):\n",
        "        x = torch.cat([obs, act], dim=-1)\n",
        "        return self.q1(x), self.q2(x)"
      ],
      "metadata": {
        "id": "q8P22VHh62j0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.buffer.append(tuple(args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
        "        return (\n",
        "            torch.tensor(states, dtype=torch.float32),\n",
        "            torch.tensor(actions, dtype=torch.float32),\n",
        "            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1),\n",
        "            torch.tensor(next_states, dtype=torch.float32),\n",
        "            torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
        "        )"
      ],
      "metadata": {
        "id": "Yt5KVfiq65_V"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# В блоке:\n",
        "env = gym.make(\"LunarLanderContinuous-v3\")\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "action_low, action_high = float(env.action_space.low[0]), float(env.action_space.high[0])\n",
        "act_limit = action_high  # Устанавливаем act_limit как максимальное значение действия (1.0 для LunarLander)\n",
        "\n",
        "actor = Actor(obs_dim, act_dim, act_limit)  # Передаем act_limit в Actor\n",
        "critic = Critic(obs_dim, act_dim)\n",
        "critic_target = Critic(obs_dim, act_dim)\n",
        "critic_target.load_state_dict(critic.state_dict())\n",
        "\n",
        "actor_opt = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
        "critic_opt = optim.Adam(critic.parameters(), lr=CRITIC_LR)\n",
        "\n",
        "replay = ReplayBuffer(REPLAY_SIZE)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "actor.to(device)\n",
        "critic.to(device)\n",
        "critic_target.to(device)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "episode_return, episode_len = 0, 0"
      ],
      "metadata": {
        "id": "AZ5gtdcF69dS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь в базовой реализации была ошибка\n",
        "```\n",
        "---------------------------------------------------------------------------\n",
        "NameError                                 Traceback (most recent call last)\n",
        "<ipython-input-8-961d9f7e8089> in <cell line: 0>()\n",
        "      4 action_low, action_high = float(env.action_space.low[0]), float(env.action_space.high[0])\n",
        "      5\n",
        "----> 6 actor = Actor(obs_dim, act_dim, act_limit)\n",
        "      7 critic = Critic(obs_dim, act_dim)\n",
        "      8 critic_target = Critic(obs_dim, act_dim)\n",
        "\n",
        "NameError: name 'act_limit' is not defined\n",
        "```\n",
        "\n",
        "пришлось править и в других местах (или тз некорректно было, не знаю, или я не понял)\n",
        "\n"
      ],
      "metadata": {
        "id": "8tnfWXULexOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update():\n",
        "    if len(replay.buffer) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    states, actions, rewards, next_states, dones = replay.sample(BATCH_SIZE)\n",
        "    states = states.to(device)\n",
        "    actions = actions.to(device)\n",
        "    rewards = rewards.to(device)\n",
        "    next_states = next_states.to(device)\n",
        "    dones = dones.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        next_actions, next_log_probs = actor(next_states)\n",
        "        q1_next, q2_next = critic_target(next_states, next_actions)\n",
        "        q_next = torch.min(q1_next, q2_next) - ALPHA * next_log_probs\n",
        "        q_target = rewards + (1 - dones) * GAMMA * q_next\n",
        "\n",
        "    q1, q2 = critic(states, actions)\n",
        "    critic_loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n",
        "    critic_opt.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_opt.step()\n",
        "\n",
        "    actions_pred, log_probs = actor(states)\n",
        "    q1_pred, q2_pred = critic(states, actions_pred)\n",
        "    q_pred = torch.min(q1_pred, q2_pred)\n",
        "    actor_loss = (ALPHA * log_probs - q_pred).mean()\n",
        "    actor_opt.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_opt.step()\n",
        "\n",
        "    for param, target_param in zip(critic.parameters(), critic_target.parameters()):\n",
        "        target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n"
      ],
      "metadata": {
        "id": "J3k7gcUKgGEi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Добавьте это в начало кода\n",
        "\n",
        "for step in range(TOTAL_STEPS):\n",
        "    if step < START_STEPS:\n",
        "        act = env.action_space.sample()\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            obs_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            act = actor.get_action(obs_t).cpu().numpy()[0]\n",
        "\n",
        "    next_obs, rew, terminated, truncated, _ = env.step(act)\n",
        "    done = terminated or truncated\n",
        "    replay.add(obs, act, rew, next_obs, done)\n",
        "\n",
        "    obs = next_obs\n",
        "    episode_return += rew\n",
        "    episode_len += 1\n",
        "\n",
        "    if done:\n",
        "        obs, _ = env.reset()\n",
        "        print(f\"Step: {step}, Return: {episode_return:.2f}, Len: {episode_len}\")\n",
        "        episode_return, episode_len = 0, 0\n",
        "\n",
        "    if step >= UPDATE_AFTER and step % UPDATE_EVERY == 0:\n",
        "        for _ in range(UPDATE_EVERY):\n",
        "            update()"
      ],
      "metadata": {
        "id": "an3ShfO27FWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0617397-482d-43f4-e0d4-c75e4e330c14"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 96, Return: -176.49, Len: 97\n",
            "Step: 194, Return: -499.83, Len: 98\n",
            "Step: 281, Return: -178.87, Len: 87\n",
            "Step: 383, Return: -302.28, Len: 102\n",
            "Step: 465, Return: 4.09, Len: 82\n",
            "Step: 545, Return: -32.39, Len: 80\n",
            "Step: 655, Return: -401.75, Len: 110\n",
            "Step: 736, Return: -75.06, Len: 81\n",
            "Step: 888, Return: -156.39, Len: 152\n",
            "Step: 963, Return: -308.58, Len: 75\n",
            "Step: 1052, Return: -391.04, Len: 89\n",
            "Step: 1217, Return: -29.32, Len: 165\n",
            "Step: 1359, Return: -413.78, Len: 142\n",
            "Step: 1498, Return: -186.85, Len: 139\n",
            "Step: 1650, Return: -86.49, Len: 152\n",
            "Step: 1724, Return: -30.84, Len: 74\n",
            "Step: 1809, Return: -58.19, Len: 85\n",
            "Step: 2809, Return: 63.31, Len: 1000\n",
            "Step: 2910, Return: -57.12, Len: 101\n",
            "Step: 2987, Return: -70.56, Len: 77\n",
            "Step: 3126, Return: -456.99, Len: 139\n",
            "Step: 3246, Return: -129.30, Len: 120\n",
            "Step: 3323, Return: -68.21, Len: 77\n",
            "Step: 3506, Return: -208.90, Len: 183\n",
            "Step: 3630, Return: -179.90, Len: 124\n",
            "Step: 3748, Return: -467.92, Len: 118\n",
            "Step: 3856, Return: -66.26, Len: 108\n",
            "Step: 3933, Return: -44.47, Len: 77\n",
            "Step: 4051, Return: -380.08, Len: 118\n",
            "Step: 4177, Return: -305.15, Len: 126\n",
            "Step: 4266, Return: -36.60, Len: 89\n",
            "Step: 4397, Return: -99.09, Len: 131\n",
            "Step: 4486, Return: -102.37, Len: 89\n",
            "Step: 4598, Return: -88.09, Len: 112\n",
            "Step: 4692, Return: -193.17, Len: 94\n",
            "Step: 4773, Return: -112.20, Len: 81\n",
            "Step: 5005, Return: -184.76, Len: 232\n",
            "Step: 5125, Return: -366.19, Len: 120\n",
            "Step: 5230, Return: -72.96, Len: 105\n",
            "Step: 5371, Return: -159.82, Len: 141\n",
            "Step: 5474, Return: -168.73, Len: 103\n",
            "Step: 5555, Return: -178.15, Len: 81\n",
            "Step: 5645, Return: -413.30, Len: 90\n",
            "Step: 5748, Return: -382.01, Len: 103\n",
            "Step: 5878, Return: -297.17, Len: 130\n",
            "Step: 5961, Return: -54.08, Len: 83\n",
            "Step: 6078, Return: -191.61, Len: 117\n",
            "Step: 6190, Return: -146.95, Len: 112\n",
            "Step: 6351, Return: -317.93, Len: 161\n",
            "Step: 6474, Return: -389.07, Len: 123\n",
            "Step: 6574, Return: -458.66, Len: 100\n",
            "Step: 6685, Return: -74.36, Len: 111\n",
            "Step: 6782, Return: -293.45, Len: 97\n",
            "Step: 6860, Return: -134.44, Len: 78\n",
            "Step: 7019, Return: -229.15, Len: 159\n",
            "Step: 7103, Return: -382.82, Len: 84\n",
            "Step: 7225, Return: -120.96, Len: 122\n",
            "Step: 7317, Return: -323.20, Len: 92\n",
            "Step: 7426, Return: -382.10, Len: 109\n",
            "Step: 7548, Return: -98.22, Len: 122\n",
            "Step: 7645, Return: -255.31, Len: 97\n",
            "Step: 7805, Return: -331.15, Len: 160\n",
            "Step: 7880, Return: -71.02, Len: 75\n",
            "Step: 8001, Return: -304.74, Len: 121\n",
            "Step: 8136, Return: -329.44, Len: 135\n",
            "Step: 8242, Return: -181.02, Len: 106\n",
            "Step: 8397, Return: -175.78, Len: 155\n",
            "Step: 8492, Return: -308.38, Len: 95\n",
            "Step: 8632, Return: -165.94, Len: 140\n",
            "Step: 8708, Return: -128.35, Len: 76\n",
            "Step: 8807, Return: -235.32, Len: 99\n",
            "Step: 8914, Return: -337.41, Len: 107\n",
            "Step: 9040, Return: -84.02, Len: 126\n",
            "Step: 9140, Return: -93.35, Len: 100\n",
            "Step: 9233, Return: -259.76, Len: 93\n",
            "Step: 9387, Return: -465.26, Len: 154\n",
            "Step: 9485, Return: -55.73, Len: 98\n",
            "Step: 9570, Return: -81.47, Len: 85\n",
            "Step: 9663, Return: -303.26, Len: 93\n",
            "Step: 9823, Return: -125.11, Len: 160\n",
            "Step: 9942, Return: -130.29, Len: 119\n",
            "Step: 10513, Return: -90.21, Len: 571\n",
            "Step: 11193, Return: 140.99, Len: 680\n",
            "Step: 12193, Return: -28.89, Len: 1000\n",
            "Step: 12838, Return: -126.34, Len: 645\n",
            "Step: 13103, Return: -10.74, Len: 265\n",
            "Step: 14103, Return: 94.27, Len: 1000\n",
            "Step: 14963, Return: -89.27, Len: 860\n",
            "Step: 15187, Return: -216.87, Len: 224\n",
            "Step: 15395, Return: -58.03, Len: 208\n",
            "Step: 15584, Return: -415.43, Len: 189\n",
            "Step: 15807, Return: -8.41, Len: 223\n",
            "Step: 15991, Return: -375.93, Len: 184\n",
            "Step: 16991, Return: -5.05, Len: 1000\n",
            "Step: 17295, Return: -47.35, Len: 304\n",
            "Step: 18295, Return: -87.28, Len: 1000\n",
            "Step: 19295, Return: -38.06, Len: 1000\n",
            "Step: 20295, Return: -10.23, Len: 1000\n",
            "Step: 20549, Return: -33.81, Len: 254\n",
            "Step: 21173, Return: -96.80, Len: 624\n",
            "Step: 21672, Return: 162.75, Len: 499\n",
            "Step: 22672, Return: 5.13, Len: 1000\n",
            "Step: 23672, Return: 0.58, Len: 1000\n",
            "Step: 24672, Return: 6.44, Len: 1000\n",
            "Step: 25672, Return: -12.57, Len: 1000\n",
            "Step: 26672, Return: 15.44, Len: 1000\n",
            "Step: 26881, Return: -231.01, Len: 209\n",
            "Step: 27462, Return: 241.24, Len: 581\n",
            "Step: 28462, Return: -29.69, Len: 1000\n",
            "Step: 29462, Return: -16.32, Len: 1000\n",
            "Step: 30462, Return: -38.23, Len: 1000\n",
            "Step: 31462, Return: -32.49, Len: 1000\n",
            "Step: 32462, Return: -8.76, Len: 1000\n",
            "Step: 33462, Return: -21.57, Len: 1000\n",
            "Step: 33526, Return: -48.09, Len: 64\n",
            "Step: 34526, Return: 40.66, Len: 1000\n",
            "Step: 35526, Return: -16.26, Len: 1000\n",
            "Step: 36526, Return: -81.55, Len: 1000\n",
            "Step: 37526, Return: -13.23, Len: 1000\n",
            "Step: 38526, Return: 7.37, Len: 1000\n",
            "Step: 39526, Return: 91.47, Len: 1000\n",
            "Step: 40526, Return: -32.35, Len: 1000\n",
            "Step: 40947, Return: 251.20, Len: 421\n",
            "Step: 41441, Return: -47.84, Len: 494\n",
            "Step: 41494, Return: -81.79, Len: 53\n",
            "Step: 41849, Return: 228.37, Len: 355\n",
            "Step: 42849, Return: 3.90, Len: 1000\n",
            "Step: 43849, Return: 32.69, Len: 1000\n",
            "Step: 44849, Return: -44.14, Len: 1000\n",
            "Step: 45849, Return: 2.92, Len: 1000\n",
            "Step: 46162, Return: 217.45, Len: 313\n",
            "Step: 46690, Return: 190.26, Len: 528\n",
            "Step: 46890, Return: -190.17, Len: 200\n",
            "Step: 47890, Return: -17.81, Len: 1000\n",
            "Step: 48719, Return: 141.87, Len: 829\n",
            "Step: 49411, Return: -139.27, Len: 692\n",
            "Step: 50411, Return: -27.44, Len: 1000\n",
            "Step: 51411, Return: -4.86, Len: 1000\n",
            "Step: 52179, Return: 140.99, Len: 768\n",
            "Step: 52589, Return: 246.49, Len: 410\n",
            "Step: 53298, Return: 191.10, Len: 709\n",
            "Step: 53464, Return: -2.23, Len: 166\n",
            "Step: 54412, Return: 139.93, Len: 948\n",
            "Step: 54863, Return: 202.61, Len: 451\n",
            "Step: 55306, Return: -141.52, Len: 443\n",
            "Step: 55728, Return: 213.36, Len: 422\n",
            "Step: 56635, Return: 133.23, Len: 907\n",
            "Step: 57531, Return: 145.82, Len: 896\n",
            "Step: 58283, Return: 195.33, Len: 752\n",
            "Step: 58668, Return: -211.62, Len: 385\n",
            "Step: 58943, Return: -130.69, Len: 275\n",
            "Step: 59189, Return: 235.37, Len: 246\n",
            "Step: 60189, Return: 53.86, Len: 1000\n",
            "Step: 60541, Return: -263.14, Len: 352\n",
            "Step: 60708, Return: 240.48, Len: 167\n",
            "Step: 61366, Return: -314.39, Len: 658\n",
            "Step: 62366, Return: -66.20, Len: 1000\n",
            "Step: 62719, Return: 224.73, Len: 353\n",
            "Step: 62895, Return: 264.70, Len: 176\n",
            "Step: 63895, Return: 17.54, Len: 1000\n",
            "Step: 64895, Return: 128.52, Len: 1000\n",
            "Step: 65084, Return: 279.79, Len: 189\n",
            "Step: 65324, Return: 251.03, Len: 240\n",
            "Step: 66287, Return: 135.09, Len: 963\n",
            "Step: 67287, Return: -49.28, Len: 1000\n",
            "Step: 68190, Return: 130.10, Len: 903\n",
            "Step: 69190, Return: -47.16, Len: 1000\n",
            "Step: 69600, Return: 274.66, Len: 410\n",
            "Step: 70600, Return: -55.01, Len: 1000\n",
            "Step: 71600, Return: 3.26, Len: 1000\n",
            "Step: 71701, Return: 12.59, Len: 101\n",
            "Step: 72083, Return: 211.80, Len: 382\n",
            "Step: 72485, Return: 201.22, Len: 402\n",
            "Step: 73485, Return: -28.41, Len: 1000\n",
            "Step: 73763, Return: 240.64, Len: 278\n",
            "Step: 74239, Return: 213.10, Len: 476\n",
            "Step: 74454, Return: 252.11, Len: 215\n",
            "Step: 75002, Return: -237.64, Len: 548\n",
            "Step: 75107, Return: 5.13, Len: 105\n",
            "Step: 75379, Return: -299.71, Len: 272\n",
            "Step: 75625, Return: 239.76, Len: 246\n",
            "Step: 76271, Return: 208.17, Len: 646\n",
            "Step: 77271, Return: -47.21, Len: 1000\n",
            "Step: 78271, Return: -18.84, Len: 1000\n",
            "Step: 79271, Return: 28.55, Len: 1000\n",
            "Step: 79701, Return: 275.58, Len: 430\n",
            "Step: 79858, Return: 272.50, Len: 157\n",
            "Step: 80118, Return: -27.24, Len: 260\n",
            "Step: 80291, Return: 33.57, Len: 173\n",
            "Step: 80457, Return: 262.49, Len: 166\n",
            "Step: 80667, Return: 255.85, Len: 210\n",
            "Step: 80828, Return: 38.49, Len: 161\n",
            "Step: 81053, Return: 258.13, Len: 225\n",
            "Step: 81425, Return: 214.40, Len: 372\n",
            "Step: 81593, Return: 277.97, Len: 168\n",
            "Step: 81850, Return: 252.75, Len: 257\n",
            "Step: 82078, Return: 5.51, Len: 228\n",
            "Step: 82726, Return: 193.60, Len: 648\n",
            "Step: 83726, Return: -40.97, Len: 1000\n",
            "Step: 83924, Return: 282.35, Len: 198\n",
            "Step: 84309, Return: 226.75, Len: 385\n",
            "Step: 84586, Return: 275.43, Len: 277\n",
            "Step: 84783, Return: 240.97, Len: 197\n",
            "Step: 85783, Return: -45.97, Len: 1000\n",
            "Step: 86341, Return: -307.42, Len: 558\n",
            "Step: 87341, Return: -48.78, Len: 1000\n",
            "Step: 88341, Return: -33.77, Len: 1000\n",
            "Step: 88561, Return: 255.68, Len: 220\n",
            "Step: 88723, Return: 290.65, Len: 162\n",
            "Step: 88916, Return: 251.20, Len: 193\n",
            "Step: 89335, Return: 254.45, Len: 419\n",
            "Step: 89800, Return: 222.10, Len: 465\n",
            "Step: 90053, Return: 231.52, Len: 253\n",
            "Step: 91053, Return: -11.36, Len: 1000\n",
            "Step: 91398, Return: 266.49, Len: 345\n",
            "Step: 91588, Return: 255.80, Len: 190\n",
            "Step: 91899, Return: 238.70, Len: 311\n",
            "Step: 92095, Return: 273.74, Len: 196\n",
            "Step: 92434, Return: -250.69, Len: 339\n",
            "Step: 93434, Return: 119.64, Len: 1000\n",
            "Step: 93534, Return: 46.64, Len: 100\n",
            "Step: 94187, Return: -64.30, Len: 653\n",
            "Step: 94284, Return: 4.06, Len: 97\n",
            "Step: 94500, Return: 262.40, Len: 216\n",
            "Step: 94617, Return: 28.03, Len: 117\n",
            "Step: 94818, Return: 282.83, Len: 201\n",
            "Step: 95052, Return: 260.57, Len: 234\n",
            "Step: 96052, Return: 63.62, Len: 1000\n",
            "Step: 96475, Return: 274.53, Len: 423\n",
            "Step: 97288, Return: 246.43, Len: 813\n",
            "Step: 97624, Return: 196.11, Len: 336\n",
            "Step: 97782, Return: 18.31, Len: 158\n",
            "Step: 98172, Return: -202.23, Len: 390\n",
            "Step: 98908, Return: 161.52, Len: 736\n",
            "Step: 99140, Return: 279.67, Len: 232\n",
            "Step: 99582, Return: -203.20, Len: 442\n",
            "Step: 99800, Return: 270.69, Len: 218\n",
            "Step: 100169, Return: 274.42, Len: 369\n",
            "Step: 100355, Return: 20.42, Len: 186\n",
            "Step: 100632, Return: 266.95, Len: 277\n",
            "Step: 101025, Return: 258.07, Len: 393\n",
            "Step: 101132, Return: 44.52, Len: 107\n",
            "Step: 101348, Return: 266.12, Len: 216\n",
            "Step: 101768, Return: 191.64, Len: 420\n",
            "Step: 101942, Return: 259.11, Len: 174\n",
            "Step: 102168, Return: 266.02, Len: 226\n",
            "Step: 102436, Return: 269.85, Len: 268\n",
            "Step: 102690, Return: 289.24, Len: 254\n",
            "Step: 102907, Return: 253.82, Len: 217\n",
            "Step: 103571, Return: 254.30, Len: 664\n",
            "Step: 104021, Return: 262.22, Len: 450\n",
            "Step: 104206, Return: 252.51, Len: 185\n",
            "Step: 104877, Return: 212.30, Len: 671\n",
            "Step: 105344, Return: 204.84, Len: 467\n",
            "Step: 105457, Return: 25.31, Len: 113\n",
            "Step: 105615, Return: 38.01, Len: 158\n",
            "Step: 106155, Return: 249.05, Len: 540\n",
            "Step: 107010, Return: 191.49, Len: 855\n",
            "Step: 107247, Return: 28.42, Len: 237\n",
            "Step: 107468, Return: 278.50, Len: 221\n",
            "Step: 107585, Return: 47.76, Len: 117\n",
            "Step: 107729, Return: 1.38, Len: 144\n",
            "Step: 107942, Return: -6.31, Len: 213\n",
            "Step: 108639, Return: 199.49, Len: 697\n",
            "Step: 109361, Return: 206.30, Len: 722\n",
            "Step: 109588, Return: 269.25, Len: 227\n",
            "Step: 110293, Return: 273.87, Len: 705\n",
            "Step: 110625, Return: 275.77, Len: 332\n",
            "Step: 111418, Return: 181.23, Len: 793\n",
            "Step: 111648, Return: 279.16, Len: 230\n",
            "Step: 111901, Return: 249.39, Len: 253\n",
            "Step: 112212, Return: 287.02, Len: 311\n",
            "Step: 112519, Return: 286.68, Len: 307\n",
            "Step: 112766, Return: 276.46, Len: 247\n",
            "Step: 113766, Return: 121.82, Len: 1000\n",
            "Step: 114072, Return: -306.84, Len: 306\n",
            "Step: 114517, Return: 263.57, Len: 445\n",
            "Step: 115396, Return: 252.52, Len: 879\n",
            "Step: 115714, Return: 295.87, Len: 318\n",
            "Step: 115943, Return: 230.51, Len: 229\n",
            "Step: 116253, Return: 258.88, Len: 310\n",
            "Step: 116746, Return: 205.10, Len: 493\n",
            "Step: 117746, Return: 126.54, Len: 1000\n",
            "Step: 117994, Return: 243.92, Len: 248\n",
            "Step: 118324, Return: 248.63, Len: 330\n",
            "Step: 118794, Return: 268.90, Len: 470\n",
            "Step: 119004, Return: 17.17, Len: 210\n",
            "Step: 119356, Return: 271.38, Len: 352\n",
            "Step: 119896, Return: 286.71, Len: 540\n",
            "Step: 120246, Return: 257.14, Len: 350\n",
            "Step: 120508, Return: 232.78, Len: 262\n",
            "Step: 120999, Return: 203.37, Len: 491\n",
            "Step: 121274, Return: 285.22, Len: 275\n",
            "Step: 121551, Return: 260.14, Len: 277\n",
            "Step: 121919, Return: 293.03, Len: 368\n",
            "Step: 122268, Return: 268.39, Len: 349\n",
            "Step: 122493, Return: 266.63, Len: 225\n",
            "Step: 122702, Return: 267.57, Len: 209\n",
            "Step: 123076, Return: 209.86, Len: 374\n",
            "Step: 124076, Return: 62.86, Len: 1000\n",
            "Step: 124683, Return: 195.56, Len: 607\n",
            "Step: 125191, Return: -46.87, Len: 508\n",
            "Step: 125404, Return: 252.76, Len: 213\n",
            "Step: 125720, Return: 254.61, Len: 316\n",
            "Step: 125804, Return: 6.80, Len: 84\n",
            "Step: 126345, Return: 196.68, Len: 541\n",
            "Step: 126540, Return: 270.14, Len: 195\n",
            "Step: 126881, Return: 128.88, Len: 341\n",
            "Step: 127057, Return: 265.29, Len: 176\n",
            "Step: 127230, Return: 269.72, Len: 173\n",
            "Step: 127388, Return: -12.04, Len: 158\n",
            "Step: 128067, Return: 265.26, Len: 679\n",
            "Step: 128183, Return: 1.24, Len: 116\n",
            "Step: 128392, Return: 251.05, Len: 209\n",
            "Step: 128651, Return: 250.56, Len: 259\n",
            "Step: 128957, Return: 252.74, Len: 306\n",
            "Step: 129280, Return: 256.77, Len: 323\n",
            "Step: 129541, Return: 273.56, Len: 261\n",
            "Step: 129775, Return: 257.92, Len: 234\n",
            "Step: 130073, Return: 265.09, Len: 298\n",
            "Step: 130888, Return: 240.58, Len: 815\n",
            "Step: 131045, Return: 251.15, Len: 157\n",
            "Step: 131258, Return: 277.50, Len: 213\n",
            "Step: 131454, Return: 244.65, Len: 196\n",
            "Step: 131925, Return: 221.39, Len: 471\n",
            "Step: 132426, Return: 229.53, Len: 501\n",
            "Step: 132640, Return: 266.19, Len: 214\n",
            "Step: 132795, Return: 274.56, Len: 155\n",
            "Step: 133138, Return: 220.45, Len: 343\n",
            "Step: 133772, Return: 171.65, Len: 634\n",
            "Step: 134106, Return: 272.05, Len: 334\n",
            "Step: 134410, Return: 284.38, Len: 304\n",
            "Step: 134920, Return: -139.98, Len: 510\n",
            "Step: 135113, Return: 281.07, Len: 193\n",
            "Step: 135395, Return: 198.88, Len: 282\n",
            "Step: 135599, Return: 261.14, Len: 204\n",
            "Step: 136273, Return: 148.98, Len: 674\n",
            "Step: 136422, Return: 259.09, Len: 149\n",
            "Step: 136970, Return: 211.76, Len: 548\n",
            "Step: 137356, Return: 172.15, Len: 386\n",
            "Step: 137515, Return: 257.63, Len: 159\n",
            "Step: 137649, Return: 36.14, Len: 134\n",
            "Step: 138323, Return: -93.46, Len: 674\n",
            "Step: 138505, Return: 256.39, Len: 182\n",
            "Step: 138662, Return: 273.72, Len: 157\n",
            "Step: 138997, Return: 212.93, Len: 335\n",
            "Step: 139164, Return: 250.61, Len: 167\n",
            "Step: 139551, Return: 189.92, Len: 387\n",
            "Step: 139982, Return: 202.37, Len: 431\n",
            "Step: 140183, Return: 272.56, Len: 201\n",
            "Step: 140480, Return: 228.30, Len: 297\n",
            "Step: 140676, Return: 259.06, Len: 196\n",
            "Step: 141002, Return: 190.04, Len: 326\n",
            "Step: 141208, Return: 249.17, Len: 206\n",
            "Step: 142122, Return: 193.97, Len: 914\n",
            "Step: 143087, Return: 151.42, Len: 965\n",
            "Step: 143283, Return: 237.07, Len: 196\n",
            "Step: 143580, Return: 274.82, Len: 297\n",
            "Step: 143774, Return: 233.13, Len: 194\n",
            "Step: 143954, Return: 260.87, Len: 180\n",
            "Step: 144570, Return: 272.00, Len: 616\n",
            "Step: 144848, Return: 214.60, Len: 278\n",
            "Step: 145394, Return: 201.42, Len: 546\n",
            "Step: 145618, Return: 225.22, Len: 224\n",
            "Step: 145873, Return: 261.22, Len: 255\n",
            "Step: 146045, Return: 266.45, Len: 172\n",
            "Step: 146478, Return: 249.23, Len: 433\n",
            "Step: 146778, Return: 240.99, Len: 300\n",
            "Step: 147049, Return: 204.08, Len: 271\n",
            "Step: 147209, Return: -18.87, Len: 160\n",
            "Step: 147728, Return: 203.08, Len: 519\n",
            "Step: 147979, Return: 272.93, Len: 251\n",
            "Step: 148146, Return: 274.44, Len: 167\n",
            "Step: 148605, Return: 283.68, Len: 459\n",
            "Step: 149020, Return: 188.62, Len: 415\n",
            "Step: 149217, Return: 265.17, Len: 197\n",
            "Step: 149586, Return: 301.65, Len: 369\n",
            "Step: 149795, Return: 279.62, Len: 209\n",
            "Step: 150740, Return: 192.98, Len: 945\n",
            "Step: 150911, Return: 257.05, Len: 171\n",
            "Step: 151282, Return: 238.33, Len: 371\n",
            "Step: 151467, Return: 246.86, Len: 185\n",
            "Step: 151906, Return: 189.93, Len: 439\n",
            "Step: 152052, Return: -203.30, Len: 146\n",
            "Step: 152249, Return: 245.91, Len: 197\n",
            "Step: 152501, Return: 251.99, Len: 252\n",
            "Step: 152869, Return: 149.91, Len: 368\n",
            "Step: 153830, Return: 208.33, Len: 961\n",
            "Step: 154255, Return: 195.55, Len: 425\n",
            "Step: 154699, Return: 277.42, Len: 444\n",
            "Step: 155699, Return: 133.06, Len: 1000\n",
            "Step: 155961, Return: 267.21, Len: 262\n",
            "Step: 156174, Return: 238.32, Len: 213\n",
            "Step: 156368, Return: 254.59, Len: 194\n",
            "Step: 156540, Return: 278.82, Len: 172\n",
            "Step: 157335, Return: 240.41, Len: 795\n",
            "Step: 157903, Return: 183.20, Len: 568\n",
            "Step: 158339, Return: 205.53, Len: 436\n",
            "Step: 158521, Return: 247.11, Len: 182\n",
            "Step: 158855, Return: -295.57, Len: 334\n",
            "Step: 159105, Return: 255.00, Len: 250\n",
            "Step: 159378, Return: 271.59, Len: 273\n",
            "Step: 159871, Return: 166.59, Len: 493\n",
            "Step: 160416, Return: 170.60, Len: 545\n",
            "Step: 160901, Return: 244.31, Len: 485\n",
            "Step: 161347, Return: 237.69, Len: 446\n",
            "Step: 161710, Return: -240.71, Len: 363\n",
            "Step: 162677, Return: 165.56, Len: 967\n",
            "Step: 162882, Return: 228.36, Len: 205\n",
            "Step: 163334, Return: -379.24, Len: 452\n",
            "Step: 164334, Return: 174.34, Len: 1000\n",
            "Step: 165334, Return: -54.16, Len: 1000\n",
            "Step: 165975, Return: -152.90, Len: 641\n",
            "Step: 166165, Return: 262.04, Len: 190\n",
            "Step: 166434, Return: 296.20, Len: 269\n",
            "Step: 167112, Return: 240.85, Len: 678\n",
            "Step: 167676, Return: 233.26, Len: 564\n",
            "Step: 167926, Return: 237.21, Len: 250\n",
            "Step: 168127, Return: 257.29, Len: 201\n",
            "Step: 168293, Return: 274.87, Len: 166\n",
            "Step: 168508, Return: 242.43, Len: 215\n",
            "Step: 169508, Return: -7.21, Len: 1000\n",
            "Step: 169963, Return: 276.85, Len: 455\n",
            "Step: 170451, Return: 165.00, Len: 488\n",
            "Step: 170654, Return: 269.02, Len: 203\n",
            "Step: 170843, Return: 248.79, Len: 189\n",
            "Step: 171041, Return: 233.24, Len: 198\n",
            "Step: 171709, Return: -136.13, Len: 668\n",
            "Step: 171920, Return: 235.01, Len: 211\n",
            "Step: 172138, Return: 258.74, Len: 218\n",
            "Step: 172306, Return: 258.25, Len: 168\n",
            "Step: 172389, Return: -18.39, Len: 83\n",
            "Step: 172736, Return: 190.08, Len: 347\n",
            "Step: 173123, Return: 237.16, Len: 387\n",
            "Step: 173305, Return: 272.06, Len: 182\n",
            "Step: 173501, Return: 247.33, Len: 196\n",
            "Step: 173684, Return: 260.09, Len: 183\n",
            "Step: 174242, Return: 217.86, Len: 558\n",
            "Step: 174567, Return: 252.46, Len: 325\n",
            "Step: 174890, Return: -247.58, Len: 323\n",
            "Step: 175375, Return: 201.92, Len: 485\n",
            "Step: 175556, Return: 243.12, Len: 181\n",
            "Step: 175813, Return: 290.61, Len: 257\n",
            "Step: 176156, Return: 196.17, Len: 343\n",
            "Step: 176262, Return: 4.78, Len: 106\n",
            "Step: 176371, Return: 2.06, Len: 109\n",
            "Step: 176591, Return: 274.64, Len: 220\n",
            "Step: 176800, Return: 279.36, Len: 209\n",
            "Step: 177226, Return: 193.18, Len: 426\n",
            "Step: 178226, Return: 156.97, Len: 1000\n",
            "Step: 178490, Return: 288.08, Len: 264\n",
            "Step: 178939, Return: 178.02, Len: 449\n",
            "Step: 179163, Return: 240.97, Len: 224\n",
            "Step: 179462, Return: 265.45, Len: 299\n",
            "Step: 179667, Return: 253.63, Len: 205\n",
            "Step: 179903, Return: 239.43, Len: 236\n",
            "Step: 180110, Return: 245.74, Len: 207\n",
            "Step: 180347, Return: 242.09, Len: 237\n",
            "Step: 180653, Return: 273.25, Len: 306\n",
            "Step: 181102, Return: 191.76, Len: 449\n",
            "Step: 181290, Return: 250.44, Len: 188\n",
            "Step: 181695, Return: 166.32, Len: 405\n",
            "Step: 182270, Return: 163.76, Len: 575\n",
            "Step: 182515, Return: 273.26, Len: 245\n",
            "Step: 182740, Return: 274.84, Len: 225\n",
            "Step: 183005, Return: 208.35, Len: 265\n",
            "Step: 183361, Return: 258.73, Len: 356\n",
            "Step: 183572, Return: 251.20, Len: 211\n",
            "Step: 184190, Return: 201.19, Len: 618\n",
            "Step: 184446, Return: 274.34, Len: 256\n",
            "Step: 184798, Return: 274.05, Len: 352\n",
            "Step: 185296, Return: 189.37, Len: 498\n",
            "Step: 185639, Return: 274.49, Len: 343\n",
            "Step: 186377, Return: 177.00, Len: 738\n",
            "Step: 187092, Return: 193.87, Len: 715\n",
            "Step: 187287, Return: 268.39, Len: 195\n",
            "Step: 187558, Return: 284.11, Len: 271\n",
            "Step: 188153, Return: 144.72, Len: 595\n",
            "Step: 188401, Return: 272.58, Len: 248\n",
            "Step: 188651, Return: 265.99, Len: 250\n",
            "Step: 188946, Return: 274.78, Len: 295\n",
            "Step: 189297, Return: 251.42, Len: 351\n",
            "Step: 189486, Return: 267.46, Len: 189\n",
            "Step: 189697, Return: 271.21, Len: 211\n",
            "Step: 189914, Return: 196.66, Len: 217\n",
            "Step: 190117, Return: 235.61, Len: 203\n",
            "Step: 190407, Return: 267.07, Len: 290\n",
            "Step: 190643, Return: 291.76, Len: 236\n",
            "Step: 190967, Return: 278.13, Len: 324\n",
            "Step: 191538, Return: 181.74, Len: 571\n",
            "Step: 191797, Return: 295.30, Len: 259\n",
            "Step: 192017, Return: 239.79, Len: 220\n",
            "Step: 192325, Return: 284.81, Len: 308\n",
            "Step: 193314, Return: 211.44, Len: 989\n",
            "Step: 193572, Return: 241.36, Len: 258\n",
            "Step: 193856, Return: 253.82, Len: 284\n",
            "Step: 194035, Return: 254.06, Len: 179\n",
            "Step: 194292, Return: 292.84, Len: 257\n",
            "Step: 194574, Return: 270.58, Len: 282\n",
            "Step: 194992, Return: 223.14, Len: 418\n",
            "Step: 195544, Return: 241.33, Len: 552\n",
            "Step: 195732, Return: 244.26, Len: 188\n",
            "Step: 196003, Return: 298.03, Len: 271\n",
            "Step: 196488, Return: 214.51, Len: 485\n",
            "Step: 196677, Return: 243.31, Len: 189\n",
            "Step: 196873, Return: 270.87, Len: 196\n",
            "Step: 197129, Return: 258.03, Len: 256\n",
            "Step: 197313, Return: 259.41, Len: 184\n",
            "Step: 197604, Return: 287.09, Len: 291\n",
            "Step: 197816, Return: 257.49, Len: 212\n",
            "Step: 198000, Return: 256.80, Len: 184\n",
            "Step: 198272, Return: 269.84, Len: 272\n",
            "Step: 198435, Return: 280.36, Len: 163\n",
            "Step: 198902, Return: 183.68, Len: 467\n",
            "Step: 199095, Return: 288.50, Len: 193\n",
            "Step: 199294, Return: 277.15, Len: 199\n",
            "Step: 199605, Return: 259.36, Len: 311\n",
            "Step: 199828, Return: 259.22, Len: 223\n"
          ]
        }
      ]
    }
  ]
}