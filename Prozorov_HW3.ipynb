{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EugIva/ProzorovEI209M_RL/blob/main/Prozorov_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Выполнил Прозоров Евгений 209М"
      ],
      "metadata": {
        "id": "UaKjnyW0UDU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализуйте алгоритм GAIL на среде Mountain Car. Перед этим сгенерируйте экспертные данные (из детерминированной стратегии с первой практики). Хорошей идеей будет добавить в state (observation) синус и косинус от временной метки t для лучшего обучения."
      ],
      "metadata": {
        "id": "EPGPfi8EajGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque\n",
        "import random"
      ],
      "metadata": {
        "id": "L1-l3BqFbaUV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_expert = gym.make(\"MountainCar-v0\")\n",
        "# Генерация экспертных данных:\n",
        "states = []\n",
        "actions = []\n",
        "\n",
        "for _ in range(1000):\n",
        "    obs, _ = env_expert.reset()\n",
        "    done = False\n",
        "    t = 0  # Инициализация t для каждого эпизода\n",
        "\n",
        "    while not done:\n",
        "        position, velocity = obs\n",
        "        if velocity < 0 or position < -0.5:\n",
        "            action = 0\n",
        "        else:\n",
        "            action = 2\n",
        "        states.append(np.concatenate([obs, [np.sin(t), np.cos(t)]]))\n",
        "        actions.append(action)\n",
        "\n",
        "        next_obs, _, terminated, truncated, _ = env_expert.step(action)\n",
        "        done = terminated or truncated\n",
        "        obs = next_obs\n",
        "        t += 1\n",
        "        # Детерминированная стратегия: выбрать действие с наименьшим ускорением (0 - слева, 1 - нейтрально, 2 - справа)\n",
        "        # Для MountainCar оптимально выбрать 2 (вправо) при положительной скорости и 0 (влево) при отрицательной\n",
        "\n",
        "expert_obs = np.array(states)\n",
        "expert_acts = np.array(actions)\n",
        "\n",
        "obs_dim = 4  # 2 исходных признака + sin(t) + cos(t)\n",
        "act_dim = 3  # Действий 3 (лево, нейтрально, право)"
      ],
      "metadata": {
        "id": "FWlezxzcPzbT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#expert_obs = np.copy(states)\n",
        "#expert_acts = np.copy(actions)"
      ],
      "metadata": {
        "id": "wyLq9c-2bvXE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 64), nn.ReLU(),\n",
        "            nn.Linear(64, act_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        logits = self.net(obs)\n",
        "        return Categorical(logits=logits)\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        dist = self.forward(obs)\n",
        "        return dist.sample().item()"
      ],
      "metadata": {
        "id": "Wp56QsLpcT0M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim + act_dim, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, obs, act):\n",
        "        act_onehot = F.one_hot(act, num_classes=3).float()  # исправлено на 3\n",
        "        x = torch.cat([obs, act_onehot], dim=1)\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "PYe2ekNUcUf5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrajectoryBuffer:\n",
        "    def __init__(self):\n",
        "        self.obs, self.acts, self.rews = [], [], []\n",
        "\n",
        "    def store(self, o, a, r):\n",
        "        self.obs.append(o)\n",
        "        self.acts.append(a)\n",
        "        self.rews.append(r)\n",
        "\n",
        "    def get(self):\n",
        "        return (\n",
        "            torch.tensor(np.array(self.obs), dtype=torch.float32),\n",
        "            torch.tensor(np.array(self.acts), dtype=torch.long),\n",
        "            torch.tensor(np.array(self.rews), dtype=torch.float32)\n",
        "        )"
      ],
      "metadata": {
        "id": "cV5NtwYMccNS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"MountainCar-v0\")\n",
        "policy = Policy(obs_dim, act_dim)\n",
        "discrim = Discriminator(obs_dim, act_dim)\n",
        "\n",
        "policy_opt = optim.Adam(policy.parameters(), lr=1e-3)\n",
        "discrim_opt = optim.Adam(discrim.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "MxfjF-ZjceE7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3000):\n",
        "    buf = TrajectoryBuffer()\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    t = 0\n",
        "\n",
        "    while not done:\n",
        "        current_obs = obs\n",
        "        extended_obs = np.concatenate([current_obs, [np.sin(t), np.cos(t)]])\n",
        "        obs_tensor = torch.tensor(extended_obs, dtype=torch.float32).unsqueeze(0)\n",
        "        action = policy.get_action(obs_tensor)\n",
        "        next_obs, _, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        buf.store(extended_obs, action, 0)\n",
        "        obs = next_obs\n",
        "        t += 1\n",
        "\n",
        "    agent_obs, agent_acts, _ = buf.get()\n",
        "\n",
        "    idxs = np.random.choice(len(expert_obs), len(agent_obs), replace=False)\n",
        "    exp_obs = torch.tensor(expert_obs[idxs], dtype=torch.float32)\n",
        "    exp_acts = torch.tensor(expert_acts[idxs], dtype=torch.long)\n",
        "\n",
        "    # Обучение дискриминатора\n",
        "    for _ in range(2):\n",
        "        discrim_opt.zero_grad()\n",
        "        disc_agent = discrim(agent_obs, agent_acts)\n",
        "        disc_expert = discrim(exp_obs, exp_acts)\n",
        "        disc_loss = F.binary_cross_entropy(disc_agent, torch.ones_like(disc_agent)) + \\\n",
        "                    F.binary_cross_entropy(disc_expert, torch.zeros_like(disc_expert))\n",
        "        disc_loss.backward()\n",
        "        discrim_opt.step()\n",
        "\n",
        "#В GAIL награда для политики должна быть log(1 - D(s,a)), а не -log(D(s,a))\n",
        "    # Вычисление награды\n",
        "    with torch.no_grad():\n",
        "        disc_agent = discrim(agent_obs, agent_acts)\n",
        "        rewards = (torch.log(1 - disc_agent + 1e-8)).cpu().numpy().flatten()\n",
        "\n",
        "    # Обновление политики\n",
        "    policy_opt.zero_grad()\n",
        "    dist = policy(agent_obs)\n",
        "    log_probs = dist.log_prob(agent_acts)\n",
        "    policy_loss = - (log_probs * torch.from_numpy(rewards).float()).mean()\n",
        "    policy_loss.backward()\n",
        "    policy_opt.step()\n",
        "\n",
        "\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: GAIL Loss {policy_loss.item():.3f}, Disc Loss {disc_loss.item():.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "R5rKCyC0ch0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb1e0c2-02c8-4545-b0ae-b84f529e3f6d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: GAIL Loss -0.745, Disc Loss 1.406\n",
            "Epoch 10: GAIL Loss -0.763, Disc Loss 1.353\n",
            "Epoch 20: GAIL Loss -0.830, Disc Loss 1.302\n",
            "Epoch 30: GAIL Loss -0.862, Disc Loss 1.266\n",
            "Epoch 40: GAIL Loss -0.816, Disc Loss 1.290\n",
            "Epoch 50: GAIL Loss -0.944, Disc Loss 1.287\n",
            "Epoch 60: GAIL Loss -0.847, Disc Loss 1.298\n",
            "Epoch 70: GAIL Loss -0.703, Disc Loss 1.346\n",
            "Epoch 80: GAIL Loss -0.694, Disc Loss 1.330\n",
            "Epoch 90: GAIL Loss -0.549, Disc Loss 1.346\n",
            "Epoch 100: GAIL Loss -0.419, Disc Loss 1.337\n",
            "Epoch 110: GAIL Loss -0.378, Disc Loss 1.326\n",
            "Epoch 120: GAIL Loss -0.319, Disc Loss 1.336\n",
            "Epoch 130: GAIL Loss -0.314, Disc Loss 1.306\n",
            "Epoch 140: GAIL Loss -0.205, Disc Loss 1.338\n",
            "Epoch 150: GAIL Loss -0.231, Disc Loss 1.325\n",
            "Epoch 160: GAIL Loss -0.346, Disc Loss 1.313\n",
            "Epoch 170: GAIL Loss -0.345, Disc Loss 1.325\n",
            "Epoch 180: GAIL Loss -0.283, Disc Loss 1.314\n",
            "Epoch 190: GAIL Loss -0.384, Disc Loss 1.302\n",
            "Epoch 200: GAIL Loss -0.389, Disc Loss 1.298\n",
            "Epoch 210: GAIL Loss -0.402, Disc Loss 1.277\n",
            "Epoch 220: GAIL Loss -0.418, Disc Loss 1.288\n",
            "Epoch 230: GAIL Loss -0.278, Disc Loss 1.287\n",
            "Epoch 240: GAIL Loss -0.413, Disc Loss 1.299\n",
            "Epoch 250: GAIL Loss -0.398, Disc Loss 1.282\n",
            "Epoch 260: GAIL Loss -0.473, Disc Loss 1.262\n",
            "Epoch 270: GAIL Loss -0.249, Disc Loss 1.327\n",
            "Epoch 280: GAIL Loss -0.230, Disc Loss 1.264\n",
            "Epoch 290: GAIL Loss -0.328, Disc Loss 1.218\n",
            "Epoch 300: GAIL Loss -0.272, Disc Loss 1.301\n",
            "Epoch 310: GAIL Loss -0.178, Disc Loss 1.255\n",
            "Epoch 320: GAIL Loss -0.256, Disc Loss 1.201\n",
            "Epoch 330: GAIL Loss -0.246, Disc Loss 1.226\n",
            "Epoch 340: GAIL Loss -0.284, Disc Loss 1.299\n",
            "Epoch 350: GAIL Loss -0.184, Disc Loss 1.240\n",
            "Epoch 360: GAIL Loss -0.171, Disc Loss 1.288\n",
            "Epoch 370: GAIL Loss -0.186, Disc Loss 1.240\n",
            "Epoch 380: GAIL Loss -0.235, Disc Loss 1.191\n",
            "Epoch 390: GAIL Loss -0.193, Disc Loss 1.185\n",
            "Epoch 400: GAIL Loss -0.189, Disc Loss 1.176\n",
            "Epoch 410: GAIL Loss -0.119, Disc Loss 1.253\n",
            "Epoch 420: GAIL Loss -0.354, Disc Loss 1.322\n",
            "Epoch 430: GAIL Loss -0.061, Disc Loss 1.242\n",
            "Epoch 440: GAIL Loss -0.120, Disc Loss 1.229\n",
            "Epoch 450: GAIL Loss -0.128, Disc Loss 1.190\n",
            "Epoch 460: GAIL Loss -0.188, Disc Loss 1.166\n",
            "Epoch 470: GAIL Loss -0.246, Disc Loss 1.195\n",
            "Epoch 480: GAIL Loss -0.213, Disc Loss 1.222\n",
            "Epoch 490: GAIL Loss -0.425, Disc Loss 1.109\n",
            "Epoch 500: GAIL Loss -0.139, Disc Loss 1.244\n",
            "Epoch 510: GAIL Loss -0.114, Disc Loss 1.223\n",
            "Epoch 520: GAIL Loss -0.060, Disc Loss 1.206\n",
            "Epoch 530: GAIL Loss -0.071, Disc Loss 1.115\n",
            "Epoch 540: GAIL Loss -0.058, Disc Loss 1.128\n",
            "Epoch 550: GAIL Loss -0.028, Disc Loss 1.144\n",
            "Epoch 560: GAIL Loss -0.068, Disc Loss 1.268\n",
            "Epoch 570: GAIL Loss -0.070, Disc Loss 1.279\n",
            "Epoch 580: GAIL Loss -0.176, Disc Loss 1.250\n",
            "Epoch 590: GAIL Loss -0.331, Disc Loss 1.166\n",
            "Epoch 600: GAIL Loss -0.110, Disc Loss 1.151\n",
            "Epoch 610: GAIL Loss -0.125, Disc Loss 1.217\n",
            "Epoch 620: GAIL Loss -0.209, Disc Loss 1.136\n",
            "Epoch 630: GAIL Loss -0.190, Disc Loss 1.188\n",
            "Epoch 640: GAIL Loss -0.093, Disc Loss 1.128\n",
            "Epoch 650: GAIL Loss -0.121, Disc Loss 1.218\n",
            "Epoch 660: GAIL Loss -0.099, Disc Loss 1.176\n",
            "Epoch 670: GAIL Loss -0.060, Disc Loss 1.169\n",
            "Epoch 680: GAIL Loss -0.170, Disc Loss 1.121\n",
            "Epoch 690: GAIL Loss -0.053, Disc Loss 1.143\n",
            "Epoch 700: GAIL Loss -0.115, Disc Loss 1.091\n",
            "Epoch 710: GAIL Loss -0.073, Disc Loss 1.146\n",
            "Epoch 720: GAIL Loss -0.039, Disc Loss 1.267\n",
            "Epoch 730: GAIL Loss -0.072, Disc Loss 1.132\n",
            "Epoch 740: GAIL Loss -0.024, Disc Loss 1.227\n",
            "Epoch 750: GAIL Loss -0.149, Disc Loss 1.104\n",
            "Epoch 760: GAIL Loss -0.086, Disc Loss 1.133\n",
            "Epoch 770: GAIL Loss -0.019, Disc Loss 1.234\n",
            "Epoch 780: GAIL Loss -0.091, Disc Loss 1.146\n",
            "Epoch 790: GAIL Loss -0.141, Disc Loss 1.126\n",
            "Epoch 800: GAIL Loss -0.091, Disc Loss 1.181\n",
            "Epoch 810: GAIL Loss -0.124, Disc Loss 1.128\n",
            "Epoch 820: GAIL Loss -0.035, Disc Loss 1.211\n",
            "Epoch 830: GAIL Loss -0.022, Disc Loss 1.146\n",
            "Epoch 840: GAIL Loss -0.069, Disc Loss 1.191\n",
            "Epoch 850: GAIL Loss -0.155, Disc Loss 1.087\n",
            "Epoch 860: GAIL Loss -0.061, Disc Loss 1.190\n",
            "Epoch 870: GAIL Loss -0.012, Disc Loss 1.317\n",
            "Epoch 880: GAIL Loss -0.016, Disc Loss 1.180\n",
            "Epoch 890: GAIL Loss -0.063, Disc Loss 1.147\n",
            "Epoch 900: GAIL Loss -0.008, Disc Loss 1.113\n",
            "Epoch 910: GAIL Loss -0.041, Disc Loss 1.286\n",
            "Epoch 920: GAIL Loss -0.066, Disc Loss 1.149\n",
            "Epoch 930: GAIL Loss -0.007, Disc Loss 1.110\n",
            "Epoch 940: GAIL Loss -0.035, Disc Loss 1.184\n",
            "Epoch 950: GAIL Loss -0.017, Disc Loss 1.110\n",
            "Epoch 960: GAIL Loss -0.008, Disc Loss 1.248\n",
            "Epoch 970: GAIL Loss -0.013, Disc Loss 1.164\n",
            "Epoch 980: GAIL Loss -0.008, Disc Loss 1.142\n",
            "Epoch 990: GAIL Loss -0.012, Disc Loss 1.133\n",
            "Epoch 1000: GAIL Loss -0.063, Disc Loss 1.140\n",
            "Epoch 1010: GAIL Loss -0.020, Disc Loss 1.194\n",
            "Epoch 1020: GAIL Loss -0.079, Disc Loss 1.181\n",
            "Epoch 1030: GAIL Loss -0.083, Disc Loss 1.248\n",
            "Epoch 1040: GAIL Loss -0.101, Disc Loss 1.122\n",
            "Epoch 1050: GAIL Loss -0.007, Disc Loss 1.137\n",
            "Epoch 1060: GAIL Loss -0.017, Disc Loss 1.177\n",
            "Epoch 1070: GAIL Loss -0.019, Disc Loss 1.130\n",
            "Epoch 1080: GAIL Loss -0.074, Disc Loss 1.135\n",
            "Epoch 1090: GAIL Loss -0.127, Disc Loss 1.102\n",
            "Epoch 1100: GAIL Loss -0.009, Disc Loss 1.228\n",
            "Epoch 1110: GAIL Loss -0.008, Disc Loss 1.116\n",
            "Epoch 1120: GAIL Loss -0.012, Disc Loss 1.139\n",
            "Epoch 1130: GAIL Loss -0.055, Disc Loss 1.139\n",
            "Epoch 1140: GAIL Loss -0.017, Disc Loss 1.112\n",
            "Epoch 1150: GAIL Loss -0.049, Disc Loss 1.073\n",
            "Epoch 1160: GAIL Loss -0.012, Disc Loss 1.172\n",
            "Epoch 1170: GAIL Loss -0.014, Disc Loss 1.245\n",
            "Epoch 1180: GAIL Loss -0.020, Disc Loss 1.146\n",
            "Epoch 1190: GAIL Loss -0.073, Disc Loss 1.268\n",
            "Epoch 1200: GAIL Loss -0.166, Disc Loss 1.147\n",
            "Epoch 1210: GAIL Loss -0.054, Disc Loss 1.125\n",
            "Epoch 1220: GAIL Loss -0.036, Disc Loss 1.161\n",
            "Epoch 1230: GAIL Loss -0.027, Disc Loss 1.134\n",
            "Epoch 1240: GAIL Loss -0.019, Disc Loss 1.303\n",
            "Epoch 1250: GAIL Loss -0.025, Disc Loss 1.164\n",
            "Epoch 1260: GAIL Loss -0.008, Disc Loss 1.220\n",
            "Epoch 1270: GAIL Loss -0.017, Disc Loss 1.108\n",
            "Epoch 1280: GAIL Loss -0.287, Disc Loss 1.172\n",
            "Epoch 1290: GAIL Loss -0.111, Disc Loss 1.122\n",
            "Epoch 1300: GAIL Loss -0.012, Disc Loss 1.100\n",
            "Epoch 1310: GAIL Loss -0.087, Disc Loss 1.080\n",
            "Epoch 1320: GAIL Loss -0.172, Disc Loss 1.183\n",
            "Epoch 1330: GAIL Loss -0.003, Disc Loss 1.092\n",
            "Epoch 1340: GAIL Loss -0.004, Disc Loss 1.190\n",
            "Epoch 1350: GAIL Loss -0.004, Disc Loss 1.249\n",
            "Epoch 1360: GAIL Loss -0.014, Disc Loss 1.140\n",
            "Epoch 1370: GAIL Loss -0.020, Disc Loss 1.130\n",
            "Epoch 1380: GAIL Loss -0.009, Disc Loss 1.108\n",
            "Epoch 1390: GAIL Loss -0.004, Disc Loss 1.151\n",
            "Epoch 1400: GAIL Loss -0.011, Disc Loss 1.214\n",
            "Epoch 1410: GAIL Loss -0.007, Disc Loss 1.112\n",
            "Epoch 1420: GAIL Loss -0.005, Disc Loss 1.174\n",
            "Epoch 1430: GAIL Loss -0.008, Disc Loss 1.140\n",
            "Epoch 1440: GAIL Loss -0.035, Disc Loss 1.236\n",
            "Epoch 1450: GAIL Loss -0.092, Disc Loss 1.076\n",
            "Epoch 1460: GAIL Loss -0.009, Disc Loss 1.136\n",
            "Epoch 1470: GAIL Loss -0.005, Disc Loss 1.090\n",
            "Epoch 1480: GAIL Loss -0.127, Disc Loss 1.098\n",
            "Epoch 1490: GAIL Loss -0.062, Disc Loss 1.259\n",
            "Epoch 1500: GAIL Loss -0.015, Disc Loss 1.105\n",
            "Epoch 1510: GAIL Loss -0.017, Disc Loss 1.267\n",
            "Epoch 1520: GAIL Loss -0.118, Disc Loss 1.078\n",
            "Epoch 1530: GAIL Loss -0.006, Disc Loss 1.159\n",
            "Epoch 1540: GAIL Loss -0.007, Disc Loss 1.178\n",
            "Epoch 1550: GAIL Loss -0.008, Disc Loss 1.115\n",
            "Epoch 1560: GAIL Loss -0.089, Disc Loss 1.126\n",
            "Epoch 1570: GAIL Loss -0.006, Disc Loss 1.145\n",
            "Epoch 1580: GAIL Loss -0.011, Disc Loss 1.143\n",
            "Epoch 1590: GAIL Loss -0.006, Disc Loss 1.091\n",
            "Epoch 1600: GAIL Loss -0.005, Disc Loss 1.107\n",
            "Epoch 1610: GAIL Loss -0.007, Disc Loss 1.214\n",
            "Epoch 1620: GAIL Loss -0.091, Disc Loss 1.102\n",
            "Epoch 1630: GAIL Loss -0.065, Disc Loss 1.108\n",
            "Epoch 1640: GAIL Loss -0.005, Disc Loss 1.195\n",
            "Epoch 1650: GAIL Loss -0.118, Disc Loss 1.116\n",
            "Epoch 1660: GAIL Loss -0.003, Disc Loss 1.082\n",
            "Epoch 1670: GAIL Loss -0.003, Disc Loss 1.112\n",
            "Epoch 1680: GAIL Loss -0.003, Disc Loss 1.130\n",
            "Epoch 1690: GAIL Loss -0.003, Disc Loss 1.061\n",
            "Epoch 1700: GAIL Loss -0.005, Disc Loss 1.201\n",
            "Epoch 1710: GAIL Loss -0.005, Disc Loss 1.252\n",
            "Epoch 1720: GAIL Loss -0.131, Disc Loss 1.127\n",
            "Epoch 1730: GAIL Loss -0.094, Disc Loss 1.131\n",
            "Epoch 1740: GAIL Loss -0.005, Disc Loss 1.087\n",
            "Epoch 1750: GAIL Loss -0.005, Disc Loss 1.114\n",
            "Epoch 1760: GAIL Loss -0.065, Disc Loss 1.098\n",
            "Epoch 1770: GAIL Loss -0.045, Disc Loss 1.176\n",
            "Epoch 1780: GAIL Loss -0.161, Disc Loss 1.253\n",
            "Epoch 1790: GAIL Loss -0.008, Disc Loss 1.155\n",
            "Epoch 1800: GAIL Loss -0.031, Disc Loss 1.110\n",
            "Epoch 1810: GAIL Loss -0.054, Disc Loss 1.081\n",
            "Epoch 1820: GAIL Loss -0.025, Disc Loss 1.091\n",
            "Epoch 1830: GAIL Loss -0.145, Disc Loss 1.100\n",
            "Epoch 1840: GAIL Loss -0.064, Disc Loss 1.067\n",
            "Epoch 1850: GAIL Loss -0.005, Disc Loss 1.204\n",
            "Epoch 1860: GAIL Loss -0.024, Disc Loss 1.149\n",
            "Epoch 1870: GAIL Loss -0.009, Disc Loss 1.245\n",
            "Epoch 1880: GAIL Loss -0.004, Disc Loss 1.153\n",
            "Epoch 1890: GAIL Loss -0.072, Disc Loss 1.058\n",
            "Epoch 1900: GAIL Loss -0.009, Disc Loss 1.166\n",
            "Epoch 1910: GAIL Loss -0.005, Disc Loss 1.175\n",
            "Epoch 1920: GAIL Loss -0.006, Disc Loss 1.139\n",
            "Epoch 1930: GAIL Loss -0.004, Disc Loss 1.065\n",
            "Epoch 1940: GAIL Loss -0.007, Disc Loss 1.221\n",
            "Epoch 1950: GAIL Loss -0.006, Disc Loss 1.089\n",
            "Epoch 1960: GAIL Loss -0.165, Disc Loss 1.116\n",
            "Epoch 1970: GAIL Loss -0.040, Disc Loss 1.125\n",
            "Epoch 1980: GAIL Loss -0.008, Disc Loss 1.263\n",
            "Epoch 1990: GAIL Loss -0.005, Disc Loss 1.053\n",
            "Epoch 2000: GAIL Loss -0.007, Disc Loss 1.119\n",
            "Epoch 2010: GAIL Loss -0.191, Disc Loss 1.239\n",
            "Epoch 2020: GAIL Loss -0.354, Disc Loss 1.118\n",
            "Epoch 2030: GAIL Loss -0.083, Disc Loss 1.112\n",
            "Epoch 2040: GAIL Loss -0.003, Disc Loss 1.086\n",
            "Epoch 2050: GAIL Loss -0.004, Disc Loss 1.140\n",
            "Epoch 2060: GAIL Loss -0.019, Disc Loss 1.115\n",
            "Epoch 2070: GAIL Loss -0.100, Disc Loss 1.102\n",
            "Epoch 2080: GAIL Loss -0.193, Disc Loss 1.183\n",
            "Epoch 2090: GAIL Loss -0.002, Disc Loss 1.162\n",
            "Epoch 2100: GAIL Loss -0.002, Disc Loss 1.133\n",
            "Epoch 2110: GAIL Loss -0.002, Disc Loss 1.142\n",
            "Epoch 2120: GAIL Loss -0.003, Disc Loss 1.176\n",
            "Epoch 2130: GAIL Loss -0.039, Disc Loss 1.097\n",
            "Epoch 2140: GAIL Loss -0.003, Disc Loss 1.144\n",
            "Epoch 2150: GAIL Loss -0.057, Disc Loss 1.044\n",
            "Epoch 2160: GAIL Loss -0.003, Disc Loss 1.085\n",
            "Epoch 2170: GAIL Loss -0.069, Disc Loss 1.081\n",
            "Epoch 2180: GAIL Loss -0.003, Disc Loss 1.085\n",
            "Epoch 2190: GAIL Loss -0.005, Disc Loss 1.243\n",
            "Epoch 2200: GAIL Loss -0.011, Disc Loss 1.169\n",
            "Epoch 2210: GAIL Loss -0.007, Disc Loss 1.175\n",
            "Epoch 2220: GAIL Loss -0.004, Disc Loss 1.084\n",
            "Epoch 2230: GAIL Loss -0.015, Disc Loss 1.050\n",
            "Epoch 2240: GAIL Loss -0.020, Disc Loss 1.096\n",
            "Epoch 2250: GAIL Loss -0.164, Disc Loss 1.084\n",
            "Epoch 2260: GAIL Loss -0.129, Disc Loss 1.022\n",
            "Epoch 2270: GAIL Loss -0.022, Disc Loss 1.124\n",
            "Epoch 2280: GAIL Loss -0.106, Disc Loss 1.071\n",
            "Epoch 2290: GAIL Loss -0.003, Disc Loss 1.141\n",
            "Epoch 2300: GAIL Loss -0.155, Disc Loss 1.136\n",
            "Epoch 2310: GAIL Loss -0.127, Disc Loss 1.101\n",
            "Epoch 2320: GAIL Loss -0.002, Disc Loss 1.201\n",
            "Epoch 2330: GAIL Loss -0.001, Disc Loss 1.093\n",
            "Epoch 2340: GAIL Loss -0.001, Disc Loss 1.110\n",
            "Epoch 2350: GAIL Loss -0.001, Disc Loss 1.117\n",
            "Epoch 2360: GAIL Loss -0.002, Disc Loss 1.241\n",
            "Epoch 2370: GAIL Loss -0.001, Disc Loss 1.135\n",
            "Epoch 2380: GAIL Loss -0.001, Disc Loss 1.119\n",
            "Epoch 2390: GAIL Loss -0.002, Disc Loss 1.052\n",
            "Epoch 2400: GAIL Loss -0.001, Disc Loss 1.075\n",
            "Epoch 2410: GAIL Loss -0.001, Disc Loss 1.121\n",
            "Epoch 2420: GAIL Loss -0.005, Disc Loss 1.227\n",
            "Epoch 2430: GAIL Loss -0.002, Disc Loss 1.140\n",
            "Epoch 2440: GAIL Loss -0.001, Disc Loss 1.068\n",
            "Epoch 2450: GAIL Loss -0.002, Disc Loss 1.204\n",
            "Epoch 2460: GAIL Loss -0.001, Disc Loss 1.095\n",
            "Epoch 2470: GAIL Loss -0.001, Disc Loss 1.074\n",
            "Epoch 2480: GAIL Loss -0.002, Disc Loss 1.095\n",
            "Epoch 2490: GAIL Loss -0.023, Disc Loss 1.079\n",
            "Epoch 2500: GAIL Loss -0.018, Disc Loss 1.138\n",
            "Epoch 2510: GAIL Loss -0.002, Disc Loss 1.101\n",
            "Epoch 2520: GAIL Loss -0.002, Disc Loss 1.098\n",
            "Epoch 2530: GAIL Loss -0.109, Disc Loss 1.136\n",
            "Epoch 2540: GAIL Loss -0.041, Disc Loss 1.089\n",
            "Epoch 2550: GAIL Loss -0.002, Disc Loss 1.131\n",
            "Epoch 2560: GAIL Loss -0.002, Disc Loss 1.097\n",
            "Epoch 2570: GAIL Loss -0.005, Disc Loss 1.149\n",
            "Epoch 2580: GAIL Loss -0.002, Disc Loss 1.002\n",
            "Epoch 2590: GAIL Loss -0.013, Disc Loss 1.206\n",
            "Epoch 2600: GAIL Loss -0.004, Disc Loss 1.120\n",
            "Epoch 2610: GAIL Loss -0.069, Disc Loss 1.093\n",
            "Epoch 2620: GAIL Loss -0.016, Disc Loss 1.223\n",
            "Epoch 2630: GAIL Loss -0.004, Disc Loss 1.129\n",
            "Epoch 2640: GAIL Loss -0.005, Disc Loss 1.238\n",
            "Epoch 2650: GAIL Loss -0.004, Disc Loss 1.149\n",
            "Epoch 2660: GAIL Loss -0.033, Disc Loss 1.058\n",
            "Epoch 2670: GAIL Loss -0.268, Disc Loss 1.146\n",
            "Epoch 2680: GAIL Loss -0.002, Disc Loss 1.161\n",
            "Epoch 2690: GAIL Loss -0.001, Disc Loss 1.098\n",
            "Epoch 2700: GAIL Loss -0.001, Disc Loss 1.026\n",
            "Epoch 2710: GAIL Loss -0.001, Disc Loss 1.195\n",
            "Epoch 2720: GAIL Loss -0.001, Disc Loss 1.011\n",
            "Epoch 2730: GAIL Loss -0.001, Disc Loss 1.048\n",
            "Epoch 2740: GAIL Loss -0.001, Disc Loss 1.125\n",
            "Epoch 2750: GAIL Loss -0.001, Disc Loss 1.073\n",
            "Epoch 2760: GAIL Loss -0.002, Disc Loss 1.170\n",
            "Epoch 2770: GAIL Loss -0.001, Disc Loss 1.116\n",
            "Epoch 2780: GAIL Loss -0.001, Disc Loss 1.074\n",
            "Epoch 2790: GAIL Loss -0.001, Disc Loss 1.069\n",
            "Epoch 2800: GAIL Loss -0.001, Disc Loss 1.131\n",
            "Epoch 2810: GAIL Loss -0.002, Disc Loss 1.149\n",
            "Epoch 2820: GAIL Loss -0.001, Disc Loss 1.046\n",
            "Epoch 2830: GAIL Loss -0.002, Disc Loss 1.131\n",
            "Epoch 2840: GAIL Loss -0.002, Disc Loss 1.197\n",
            "Epoch 2850: GAIL Loss -0.001, Disc Loss 1.089\n",
            "Epoch 2860: GAIL Loss -0.001, Disc Loss 1.135\n",
            "Epoch 2870: GAIL Loss -0.001, Disc Loss 1.072\n",
            "Epoch 2880: GAIL Loss -0.001, Disc Loss 1.092\n",
            "Epoch 2890: GAIL Loss -0.001, Disc Loss 1.011\n",
            "Epoch 2900: GAIL Loss -0.240, Disc Loss 1.107\n",
            "Epoch 2910: GAIL Loss -0.001, Disc Loss 1.132\n",
            "Epoch 2920: GAIL Loss -0.001, Disc Loss 1.139\n",
            "Epoch 2930: GAIL Loss -0.001, Disc Loss 1.058\n",
            "Epoch 2940: GAIL Loss -0.001, Disc Loss 1.118\n",
            "Epoch 2950: GAIL Loss -0.001, Disc Loss 1.030\n",
            "Epoch 2960: GAIL Loss -0.021, Disc Loss 1.052\n",
            "Epoch 2970: GAIL Loss -0.007, Disc Loss 1.197\n",
            "Epoch 2980: GAIL Loss -0.002, Disc Loss 1.075\n",
            "Epoch 2990: GAIL Loss -0.001, Disc Loss 1.099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Тестирование\n",
        "for episode in range(10):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    t = 0\n",
        "\n",
        "    while not done:\n",
        "        state = np.concatenate([obs, [np.sin(t), np.cos(t)]])\n",
        "        obs_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        action = policy.get_action(obs_tensor)\n",
        "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "        t += 1\n",
        "    print(f\"Episode {episode}: Total reward {total_reward}\")\n",
        "env.close()"
      ],
      "metadata": {
        "id": "Mm4v4361UcSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e31efcc-de6c-44b9-89f5-80377952e2c7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Total reward -200.0\n",
            "Episode 1: Total reward -200.0\n",
            "Episode 2: Total reward -200.0\n",
            "Episode 3: Total reward -200.0\n",
            "Episode 4: Total reward -200.0\n",
            "Episode 5: Total reward -200.0\n",
            "Episode 6: Total reward -200.0\n",
            "Episode 7: Total reward -200.0\n",
            "Episode 8: Total reward -200.0\n",
            "Episode 9: Total reward -200.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "что-то не получилось :("
      ],
      "metadata": {
        "id": "uIpw082uT-L9"
      }
    }
  ]
}